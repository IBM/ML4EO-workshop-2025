{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7009d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2025 International Business Machines\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#  http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bacc318390456b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook focuses on fine-tuning the [Prithvi EO v2.0 model](https://huggingface.co/collections/ibm-nasa-geospatial/prithvi-for-earth-observation-6740a7a81883466bf41d93d6) to classify crops in a HLS scene. The main take aways from this notebook will be as follows:\n",
    "1. Learn how to use Terratorch to fine-tune Prithvi EO v2.0 300m for crop classification (13 classes).\n",
    "2. Use Huggingface datasets with Prithvi EO.\n",
    "3. Understand the effects of spefic parameters in training and hardware utilisation.\n",
    "4. Use fine-tuned model for inference.\n",
    "\n",
    "You may want to take this opportunity to double check you're using GPUs on Google Colab before proceeding any further. We have tested this notebook using T4 GPU on the free colab account.\n",
    "\n",
    "## Setup\n",
    "1. Install terratorch\n",
    "\n",
    "To install the necessary packages, execute the cell below. This will take a few minutes. Once the installation process is done, a window will pop up to ask you to restart the session. This is normal and you should proceed to restart using the interface in the pop up window. Once the session has restarted, its important that you ignore the cell below, and go straight to section 0.1.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install terratorch==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c25f3b",
   "metadata": {},
   "source": [
    "2. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import albumentations\n",
    "import gdown\n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "import terratorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from pathlib import Path\n",
    "from terratorch.datamodules import MultiTemporalCropClassificationDataModule\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b65b8e7cd7d65",
   "metadata": {},
   "source": [
    "3. Download the dataset from Google Drive. To learn more about the datset used in this notebook, take a look a the Dataset card on [Hugging Face](https://huggingface.co/datasets/ibm-nasa-geospatial/multi-temporal-crop-classification). This dataset contains Harmonized Landsat-Sentinel (HLS) and crop type classes across the Contiguous United States for the year 2022. The target labels are derived from USDA's Crop Data Layer (CDL). Each labels has 3 corresponding HLS images corresponding to different times of the year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3t-YKKUztjXn",
   "metadata": {
    "id": "3t-YKKUztjXn",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_path = \"../data/multi-temporal-crop-classification\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/multi-temporal-crop-classification\",\n",
    "    allow_patterns=\"*.tgz\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=dataset_path,\n",
    ")\n",
    "snapshot_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/multi-temporal-crop-classification\",\n",
    "    allow_patterns=\"*.txt\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=dataset_path,\n",
    ")\n",
    "!mkdir ../data/multi-temporal-crop-classification/training_chips; tar -xzf ../data/multi-temporal-crop-classification/training_chips.tgz -C ../data/multi-temporal-crop-classification/\n",
    "!mkdir ../data/multi-temporal-crop-classification/validation_chips; tar -xzf ../data/multi-temporal-crop-classification/validation_chips.tgz -C ../data/multi-temporal-crop-classification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20332275",
   "metadata": {},
   "source": [
    "4. Truncate the dataset for demonstration purposes. Reducing the training dataset to a third of the original size means that model training takes only a few minutes with the resources available during the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4517aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_truncation = 800\n",
    "validation_data_trunction = 4\n",
    "with open(f\"{dataset_path}/training_data.txt\", \"r\") as f:\n",
    "      training_data_list = f.readlines()\n",
    "truncated = training_data_list[0:training_data_truncation]\n",
    "with open(f\"{dataset_path}/training_data.txt\", \"w\") as f:\n",
    "    for i in truncated:\n",
    "        f.write(i)\n",
    "\n",
    "with open(f\"{dataset_path}/validation_data.txt\", \"r\") as f:\n",
    "      training_data_list = f.readlines()\n",
    "truncated = training_data_list[0:validation_data_trunction]\n",
    "with open(f\"{dataset_path}/validation_data.txt\", \"w\") as f:\n",
    "    for i in truncated:\n",
    "        f.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## Multi-temporal Crop Dataset\n",
    "\n",
    "Lets start with analysing the dataset. \n",
    "\n",
    "Please note: we have also set the batch_size parameter to 4 and max_epochs to 1 to avoid running out of memory or runtime for users of the free tier colab compute resources. This is enough to demonstrate the entire workflow to the user, but may not result in the best performance. It'll be best to find additional compute resources and increase batch_size and max_epochs in the downloaded config file for improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd7d83440895e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each merged sample includes the stacked bands of three time steps\n",
    "!ls \"{dataset_path}/training_chips\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify parameters to select the batch size, number of workers, model backbone and epochs ahead of initalizing the MultiTemporalCropClassificationDataModule class for multi-temporal crop classification. \n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "prithvi_backbone = \"prithvi_eo_v2_300_tl\" # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "\n",
    "# Total number of epochs the training will run for.\n",
    "max_epochs =  1 # Use 1 epoch for demos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91025bb8",
   "metadata": {},
   "source": [
    "#### Initialise the Datamodules class \n",
    "\n",
    "A Datamodule is a shareable, reusable class that encapsulates all the steps needed to process the data. Here we are using an adjusted dataset class for this dataset (general dataset class could be used as well). To learn more about MultiTemporalCropClassificationDataModule, take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/datamodules/?h=multitemporalcropclassificationdatamodule#terratorch.datamodules.multi_temporal_crop_classification.MultiTemporalCropClassificationDataModule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "outputs": [],
   "source": [
    "datamodule = MultiTemporalCropClassificationDataModule(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    data_root=f\"{dataset_path}\",\n",
    "    train_transform=[\n",
    "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "    expand_temporal_dimension=True,\n",
    "    use_metadata=False, # The crop dataset has metadata for location and time\n",
    "    reduce_zero_label=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdd71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925",
   "metadata": {
    "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925"
   },
   "outputs": [],
   "source": [
    "# Mean and standard deviation calculated from the training dataset for all 6 bands, and 3 timesteps, for zero mean normalization.\n",
    "# checking for the dataset means and stds\n",
    "datamodule.means, datamodule.stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08644e71-d82f-426c-b0c1-79026fccb578",
   "metadata": {
    "id": "08644e71-d82f-426c-b0c1-79026fccb578"
   },
   "outputs": [],
   "source": [
    "# checking datasets train split size\n",
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b86821-3481-4d92-bdba-246568c66c48",
   "metadata": {
    "id": "88b86821-3481-4d92-bdba-246568c66c48"
   },
   "outputs": [],
   "source": [
    "# checking datasets available bands\n",
    "train_dataset.all_band_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9264de41-ab16-43cc-9ea2-ee51b0969624",
   "metadata": {
    "id": "9264de41-ab16-43cc-9ea2-ee51b0969624"
   },
   "outputs": [],
   "source": [
    "# checking datasets classes\n",
    "train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
    "outputId": "9c948b7c-e02b-4980-a142-b36bcb51a8e4"
   },
   "outputs": [],
   "source": [
    "# plotting a few samples\n",
    "for i in range(5):\n",
    "    train_dataset.plot(train_dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
   "metadata": {
    "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b"
   },
   "outputs": [],
   "source": [
    "# checking datasets validation split size\n",
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "outputs": [],
   "source": [
    "# checking datasets testing split size\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072e2f849c0df2d",
   "metadata": {},
   "source": [
    "# Fine-tune Prithvi\n",
    "\n",
    "Here we setup the fine-tuning including which type of task, which head to use and the model parameters. In this case we are doing segemtation task (you can take a look at this and other downstream tasks here [TerraTorch docs](https://ibm.github.io/terratorch/stable/tasks/)) and using a unet decoder. We also set the numbers of images per label with the \"backbone_num_frames\" parameter to allow us to perform multi-temporal classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../output/multicrop/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Lightning multi-gpu often fails in notebooks\n",
    "    precision='bf16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True, # Uses TensorBoard by default\n",
    "    max_epochs=max_epochs,\n",
    "    log_every_n_steps=5,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"../output/multicrop\",\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": prithvi_backbone,\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 3,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11]  # 100m model\n",
    "                \"indices\": [5, 11, 17, 23]  # 300m model\n",
    "                # \"indices\": [7, 15, 23, 31]  # 300m model\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ReshapeTokensToImage\",\n",
    "                \"effective_time_dim\": 3\n",
    "            },\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"},\n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 13,\n",
    "    },\n",
    "\n",
    "    loss=\"ce\",\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True,  # Speeds up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fee1e72be7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c27a7aa4f21ea",
   "metadata": {},
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "Let's gather and specify the relevant files for carrying out testing. Look for your .ckpt file produced during the fine-tuning process here it is in '../output/multicrop/checkpoints/best-epoch=00.ckpt'. We have also provided a model that has been trained on the full dataset so that we can compare it to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388aa3db0dc07460",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt_path = \"../output/multicrop/checkpoints/best-epoch=00.ckpt\"\n",
    "\n",
    "# Download best model checkpoint fine-tuned on full dataset\n",
    "best_ckpt_100_epoch_path = \"multicrop_best-epoch=76.ckpt\"\n",
    "\n",
    "if not os.path.isfile(best_ckpt_100_epoch_path):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1cO5a9PmV70j6mvlTc8zH8MnKsRCGbefm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test metrics\n",
    "trainer.test(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62920a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "preds = trainer.predict(model, datamodule=datamodule, ckpt_path=best_ckpt_100_epoch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfc723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data \n",
    "data_loader = trainer.predict_dataloaders\n",
    "batch = next(iter(data_loader))\n",
    "\n",
    "# plot\n",
    "for i in range(batch_size):\n",
    "    sample = {key: batch[key][i] for key in batch}\n",
    "    sample[\"prediction\"] = preds[0][0][0][i].cpu().numpy()\n",
    "\n",
    "    datamodule.predict_dataset.plot(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c88e2d5ab78020",
   "metadata": {},
   "source": [
    "# Fine-tuning via CLI\n",
    "\n",
    "We also run the fine-tuning via a [CLI](https://ibm.github.io/terratorch/stable/quick_start/#training-with-lightning-tasks). All parameteres we have specified in the notebook can be put in a [yaml]( ../configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml), and can be run using the command below. Take a look at the [TerraTorch docs](https://ibm.github.io/terratorch/stable/tutorials/the_yaml_config/) for how to setup the config.\n",
    "\n",
    "You might want to restart the session to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2553ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get the config file from github.com.\n",
    "!git init\n",
    "!git remote add origin https://github.com/IBM/ML4EO-workshop-2025.git\n",
    "!git fetch --all\n",
    "!git checkout origin/main -- \"Prithvi-EO/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbf05ebc81b9998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fine-tuning\n",
    "!terratorch fit -c \"Prithvi-EO/configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "studio_py3_11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
